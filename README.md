# ğŸ“˜ Regression CheatSheet â€” Machine Learning Regression Algorithms (with SHAP Explainability)

A clean, structured collection of **regression algorithms**, **code templates**, and **explainability tools** written in Python using scikit-learn, XGBoost, and SHAP.

This repository helps you:

- Understand **how each regression algorithm works**
- Learn **how to train, evaluate, and interpret** models
- Access **ready-to-run code templates** for real datasets or your own dataset
- Use **SHAP** to explain predictions of any regression model
- Explore models interactively using Jupyter notebooks

![Cover](https://github.com/user-attachments/assets/0a472a60-2f61-4eb2-af8f-9a2e2d11a34c) <br/>

---

## ğŸ”§ Folder Structure

```

â”œâ”€â”€ regression_cheatsheet.py         # Main script running ALL algorithms + SHAP summaries
â”œâ”€â”€ regression_cheatsheet.ipynb      # Notebook version for interactive demonstration
â”œâ”€â”€ SHAP_integration.py              # Explainability-specific file (for reference)
â”œâ”€â”€ SHAP_integration.ipynb           # Fully working SHAP explainability notebook
â”œâ”€â”€ requirements.txt                 # Dependencies
â””â”€â”€ data/ (optional)
        â””â”€â”€ your_dataset.csv         # Users may place their own CSV here

````

> **Note:**  
> SHAP visualizations require a **Jupyter environment**.  
> `SHAP_integration.py` is provided only for reference;  
> for working SHAP visualizations, use **`SHAP_integration.ipynb`**.

---

# ğŸš€ Getting Started

### Install dependencies:

Inside a Jupyter notebook:

```python
!pip install -r requirements.txt -q
````

Or from terminal:

```bash
pip install -r requirements.txt
```

### Run the complete cheat sheet:

```bash
python regression_cheatsheet.py
```

### Interactive notebook version:

```
regression_cheatsheet.ipynb
```

---

# ğŸ“Š Dataset Usage

The cheat sheet supports:

### **1. Diabetes dataset (default)**

Loaded from scikit-learn.

### **2. Custom user dataset**

If using your own dataset, place it inside:

```
data/your_file.csv
```

Then call:

```python
run_all(dataset="house")
```

Your CSV must contain:

* **3 input columns** â†’ e.g., `column1`, `column2`, `column3`
* **1 target column** â†’ e.g., `column4`

Example usage inside the code:

```python
df = pd.read_csv("data/filename.csv")
X = df[["column1", "column2", "column3"]].values
y = df["column4"].values
```

This repository does **not** include a dataset.
Users may supply any properly formatted CSV.

---

# ğŸ§  Algorithms Covered (Brief Theory + Code Snippets)

Below is a clear explanation of each algorithm, when to use it, and a short runnable snippet.

# 1ï¸âƒ£ **Linear Regression**

A simple model assuming a linear relationship between features and target.

**Good for:** <br/>
âœ” Simple relationships <br/>
âœ” Quick baseline <br/>
âœ” Interpretable coefficients <br/>

**Code:**

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression().fit(X_train, y_train)
pred = model.predict(X_test)
```

---

# 2ï¸âƒ£ **Ridge, Lasso, ElasticNet (Regularization)**

### ğŸ”¹ Ridge Regression

Adds **L2 penalty** â†’ shrinks coefficients, reduces variance.

```python
from sklearn.linear_model import Ridge
model = Ridge(alpha=1.0).fit(X_train, y_train)
```

### ğŸ”¹ Lasso Regression

Adds **L1 penalty** â†’ performs feature selection.

```python
from sklearn.linear_model import Lasso
model = Lasso(alpha=0.01).fit(X_train, y_train)
```

### ğŸ”¹ ElasticNet

Combination of L1 + L2.

```python
ElasticNet(alpha=0.01, l1_ratio=0.5)
```

---

# 3ï¸âƒ£ **Polynomial Regression**

Transforms features into higher-degree polynomial combinations.

**Useful for:** <br/>
âœ” Non-linear relationships <br/>
âœ” Smooth curves <br/>

**Code:**

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge

pipe = Pipeline([
    ("poly", PolynomialFeatures(degree=2)),
    ("ridge", Ridge())
])
pipe.fit(X_train, y_train)
```

---

# 4ï¸âƒ£ **Support Vector Regression (SVR)**

A powerful model using kernel functions to capture complex non-linear patterns.

**Pros:** <br/>
âœ” Works well on small datasets <br/>
âœ” Handles outliers using Îµ-insensitive loss <br/>

**Code:**

```python
from sklearn.svm import SVR
model = SVR(kernel="rbf").fit(X_train, y_train)
```

---

# 5ï¸âƒ£ **Decision Tree Regression**

Learns decision boundaries in feature space.

**Pros:** <br/>
âœ” Interpretable <br/>
âœ” Handles non-linearity <br/>
âœ” No scaling required <br/>

```python
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor().fit(X_train, y_train)
```

---

# 6ï¸âƒ£ **Random Forest Regression**

An ensemble of many decision trees.

**Pros:** <br/>
âœ” High accuracy <br/>
âœ” Low overfitting <br/>
âœ” Handles noisy data <br/>

```python
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor().fit(X_train, y_train)
```

---

# 7ï¸âƒ£ **Gradient Boosting Regression**

Sequential ensemble of decision trees trained to correct previous errors.

**Pros:** <br/>
âœ” Very accurate <br/>
âœ” Works well on structured/tabular data <br/>
âœ” Handles non-linearity <br/>

```python
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor().fit(X_train, y_train)
```

---

# 8ï¸âƒ£ **XGBoost Regression**

Extreme Gradient Boosting â€” optimized, regularized, fast.

**Pros:** <br/>
ğŸ”¥ State-of-the-art performance on tabular data <br/>
ğŸ”¥ Built-in regularization <br/>
ğŸ”¥ GPU acceleration <br/>

```python
import xgboost as xgb
model = xgb.XGBRegressor().fit(X_train, y_train)
```

---

# 9ï¸âƒ£ **Pipeline + Ridge Regression**

Combining preprocessing + model into a single workflow.

```python
pipe = Pipeline([
    ("scale", StandardScaler()),
    ("ridge", Ridge())
])
pipe.fit(X_train, y_train)
```

---

# ğŸ“ˆ Evaluation Metrics Used

| Metric                        | Measures                    | Notes                 |
| ----------------------------- | --------------------------- | --------------------- |
| **RÂ² Score**                  | Variance explained by model | Higher â†’ better       |
| **MSE (Mean Squared Error)**  | Squared error               | Sensitive to outliers |
| **MAE (Mean Absolute Error)** | Absolute error              | More robust           |

Example:

```python
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
```

---

# ğŸ” SHAP Explainability

Every model in the cheat sheet runs SHAP explainability.

### What SHAP provides:

* Feature importance
* Beeswarm summary plots
* Local prediction explanations

### Example:

```python
import shap
explainer = shap.TreeExplainer(model)
shap_values = explainer(X_test)
shap.plots.beeswarm(shap_values)
```

### Important:

> **SHAP visualizations only work in Jupyter.**
> For working examples, use **`SHAP_integration.ipynb`**.
> `SHAP_integration.py` is for reference only.

---

# ğŸƒ Running Everything at Once

```bash
python regression_cheatsheet.py
```

This runs:

* Linear Regression
* Ridge
* Lasso
* ElasticNet
* Polynomial Regression
* SVR
* Decision Tree
* Random Forest
* Gradient Boosting
* XGBoost
* SHAP Explainability for all models

---

# â­ Future Extensions (optional)

* Add LightGBM & CatBoost variants
* Add hyperparameter tuning (GridSearch / Optuna)
* Add Regression Comparison Dashboard
* Add more datasets (Salary, Cars, Custom Synthetic Data)

---

### ğŸ‘‹ **Goodbye Note**

Good luck, and may your RÂ² rise, your MSE fall,
and your SHAP plots always make sense.

---

### ğŸ¾ Follow Me

If you enjoyed this analysis, check out more of my work:

ğŸ”— [GitHub](https://github.com/AdityaBhatt3010) <br/>
ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/adityabhatt3010/) <br/>
âœï¸ [Medium](https://medium.com/@adityabhatt3010) <br/>

---

# ğŸ‘¨â€ğŸ’» Crafted By  

**Aditya Bhatt** â€” Turning black-box models into transparent systems.

---




