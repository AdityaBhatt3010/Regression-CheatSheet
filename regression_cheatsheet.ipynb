{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a2e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegression CheatSheet - regression_cheatsheet.py\\n\\nQuick repo starter file for your Regression CheatSheet repo.\\nIncludes runnable examples (using sklearn.datasets.load_diabetes by default),\\nand functions for:\\n - Linear, Ridge, Lasso, ElasticNet\\n - Polynomial (pipeline)\\n - SVR\\n - DecisionTreeRegressor, RandomForestRegressor, GradientBoostingRegressor\\n - Pipeline example (StandardScaler + Ridge)\\n - Optional placeholders for XGBoost/LightGBM (commented)\\n\\nHow to use:\\n - python regression_cheatsheet.py               -> runs demo on diabetes dataset\\n - python regression_cheatsheet.py --data house   -> tries to load data/house_prices_small.csv\\n - Import functions in a notebook: from regression_cheatsheet import train_and_eval_linear, load_data, ...\\n\\nRequirements (minimal):\\n numpy, pandas, scikit-learn, matplotlib (optional)\\n Optional: xgboost, lightgbm (commented)\\n\\nAuthor: Aditya Bhatt\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Regression CheatSheet - regression_cheatsheet.ipynb\n",
    "Author: Aditya Bhatt\n",
    "Date: 2024-06-15\n",
    "\n",
    "Quick repo starter file for your Regression CheatSheet repo.\n",
    "Includes runnable examples (using sklearn.datasets.load_diabetes by default),\n",
    "and functions for:\n",
    " - Linear, Ridge, Lasso, ElasticNet\n",
    " - Polynomial (pipeline)\n",
    " - SVR\n",
    " - DecisionTreeRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    " - Pipeline example (StandardScaler + Ridge)\n",
    " - Optional placeholders for XGBoost/LightGBM (commented)\n",
    "\n",
    "Requirements (minimal):\n",
    " numpy, pandas, scikit-learn, matplotlib (optional)\n",
    " Optional: xgboost, lightgbm (commented)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d1d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements.txt -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897d3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Regression models / tools\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utilities ----------\n",
    "\n",
    "def load_data(dataset: str = \"diabetes\", test_size: float = 0.2, random_state: int = 42\n",
    "             ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load dataset and split.\n",
    "    dataset: 'diabetes' or 'custom dataset' (expects data/filename.csv with say 4 columns)\n",
    "    returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    if dataset == \"house\":\n",
    "        path = os.path.join(\"data\", \"filename.csv\")\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"{path} not found. Place your CSV at this path or use --data diabetes\")\n",
    "        df = pd.read_csv(path)\n",
    "        X = df[[\"column1\", \"column2\", \"column3\"]].values\n",
    "        y = df[\"column4\"].values\n",
    "    else:\n",
    "        from sklearn.datasets import load_diabetes\n",
    "        data = load_diabetes()\n",
    "        X = data[\"data\"]\n",
    "        y = data[\"target\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def print_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray, name: str = None) -> Dict[str, float]:\n",
    "    \"\"\"Compute and print R2, MSE, MAE. Returns a dict of metrics.\"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    if name:\n",
    "        print(f\"[{name}] R2: {r2:.4f} | MSE: {mse:.4f} | MAE: {mae:.4f}\")\n",
    "    else:\n",
    "        print(f\"R2: {r2:.4f} | MSE: {mse:.4f} | MAE: {mae:.4f}\")\n",
    "    return {\"r2\": r2, \"mse\": mse, \"mae\": mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19e7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Models & examples ----------\n",
    "\n",
    "def train_and_eval_linear(X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return print_regression_metrics(y_test, y_pred, \"LinearRegression\")\n",
    "\n",
    "\n",
    "def train_and_eval_ridge_lasso_enet(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    ridge = Ridge(alpha=1.0, random_state=42).fit(X_train, y_train)\n",
    "    results[\"ridge\"] = print_regression_metrics(y_test, ridge.predict(X_test), \"Ridge\")\n",
    "\n",
    "    lasso = Lasso(alpha=0.01, random_state=42, max_iter=10000).fit(X_train, y_train)\n",
    "    results[\"lasso\"] = print_regression_metrics(y_test, lasso.predict(X_test), \"Lasso\")\n",
    "\n",
    "    enet = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42, max_iter=10000).fit(X_train, y_train)\n",
    "    results[\"elasticnet\"] = print_regression_metrics(y_test, enet.predict(X_test), \"ElasticNet\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def train_and_eval_polynomial(X_train, X_test, y_train, y_test, degree: int = 3):\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "            (\"scale\", StandardScaler(with_mean=False)),\n",
    "            (\"lin\", Ridge(alpha=1.0)),\n",
    "        ]\n",
    "    )\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    return print_regression_metrics(y_test, y_pred, f\"Polynomial(deg={degree})\")\n",
    "\n",
    "\n",
    "def train_and_eval_svr(X_train, X_test, y_train, y_test):\n",
    "    svr = SVR(kernel=\"rbf\", C=1.0, epsilon=0.1).fit(X_train, y_train)\n",
    "    y_pred = svr.predict(X_test)\n",
    "    return print_regression_metrics(y_test, y_pred, \"SVR\")\n",
    "\n",
    "\n",
    "def train_and_eval_tree_ensembles(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    dt = DecisionTreeRegressor(random_state=42).fit(X_train, y_train)\n",
    "    results[\"dt\"] = print_regression_metrics(y_test, dt.predict(X_test), \"DecisionTree\")\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "    results[\"rf\"] = print_regression_metrics(y_test, rf.predict(X_test), \"RandomForest\")\n",
    "\n",
    "    gbr = GradientBoostingRegressor(learning_rate=0.05, n_estimators=200, max_depth=3, random_state=42).fit(\n",
    "        X_train, y_train\n",
    "    )\n",
    "    results[\"gbr\"] = print_regression_metrics(y_test, gbr.predict(X_test), \"GradientBoosting\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def train_and_eval_xgboost(X_train, X_test, y_train, y_test):\n",
    "    xg = xgb.XGBRegressor(n_estimators=200, random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "    return print_regression_metrics(y_test, xg.predict(X_test), \"XGBoost\")\n",
    "\n",
    "def pipeline_ridge(X_train, X_test, y_train, y_test):\n",
    "    pipe = Pipeline([(\"scale\", StandardScaler()), (\"ridge\", Ridge(alpha=1.0))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    return print_regression_metrics(y_test, y_pred, \"Pipeline-Ridge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30337439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regression CheatSheet Demo (dataset: diabetes) ===\n",
      "[LinearRegression] R2: 0.4526 | MSE: 2900.1936 | MAE: 42.7941\n",
      "[Ridge] R2: 0.4192 | MSE: 3077.4159 | MAE: 46.1389\n",
      "[Lasso] R2: 0.4567 | MSE: 2878.5594 | MAE: 42.8318\n",
      "[ElasticNet] R2: 0.3736 | MSE: 3318.5057 | MAE: 48.3961\n",
      "[Polynomial(deg=2)] R2: 0.4558 | MSE: 2883.3630 | MAE: 41.7144\n",
      "[SVR] R2: 0.1821 | MSE: 4333.2860 | MAE: 56.0237\n",
      "[DecisionTree] R2: 0.0607 | MSE: 4976.7978 | MAE: 54.5281\n",
      "[RandomForest] R2: 0.4428 | MSE: 2952.0106 | MAE: 44.0530\n",
      "[GradientBoosting] R2: 0.4661 | MSE: 2828.5135 | MAE: 43.4206\n",
      "[XGBoost] R2: 0.3675 | MSE: 3350.8237 | MAE: 46.3867\n",
      "[Pipeline-Ridge] R2: 0.4541 | MSE: 2892.0146 | MAE: 42.8120\n"
     ]
    }
   ],
   "source": [
    "# ---------- Demo runner ----------\n",
    "\n",
    "def run_all(dataset: str = \"diabetes\"):\n",
    "    X_train, X_test, y_train, y_test = load_data(dataset=dataset)\n",
    "    print(\"\\n=== Regression CheatSheet Demo (dataset: {}) ===\".format(dataset))\n",
    "    train_and_eval_linear(X_train, X_test, y_train, y_test)\n",
    "    train_and_eval_ridge_lasso_enet(X_train, X_test, y_train, y_test)\n",
    "    train_and_eval_polynomial(X_train, X_test, y_train, y_test, degree=2)\n",
    "    train_and_eval_svr(X_train, X_test, y_train, y_test)\n",
    "    train_and_eval_tree_ensembles(X_train, X_test, y_train, y_test)\n",
    "    train_and_eval_xgboost(X_train, X_test, y_train, y_test)\n",
    "    pipeline_ridge(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
